<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="NovaVibe"><meta property="og:type" content="article"><meta name=robots content="index,follow,noarchive"><meta property="og:image" content="//img/home-bg-jeep.jpg"><meta property="twitter:image" content="//img/home-bg-jeep.jpg"><meta name=title content="Artificial Intelligence Has a Racial and Gender Bias Problem"><meta property="og:title" content="Artificial Intelligence Has a Racial and Gender Bias Problem"><meta property="twitter:title" content="Artificial Intelligence Has a Racial and Gender Bias Problem"><meta name=description content="Machines can discriminate in harmful ways. I experienced this firsthand, when I was a graduate student at MIT in 2015 and discovered that some facial analysis software couldnt detect my dark-skinned face until I put on a white mask. These systems are often trained on images of predominantly light-skinned men. And so, I decided to"><meta property="og:description" content="Machines can discriminate in harmful ways. I experienced this firsthand, when I was a graduate student at MIT in 2015 and discovered that some facial analysis software couldnt detect my dark-skinned face until I put on a white mask. These systems are often trained on images of predominantly light-skinned men. And so, I decided to"><meta property="twitter:description" content="Machines can discriminate in harmful ways. I experienced this firsthand, when I was a graduate student at MIT in 2015 and discovered that some facial analysis software couldnt detect my dark-skinned face until I put on a white mask. These systems are often trained on images of predominantly light-skinned men. And so, I decided to"><meta property="twitter:card" content="summary"><meta name=keyword content><link rel="shortcut icon" href=./img/favicon.ico><title>Artificial Intelligence Has a Racial and Gender Bias Problem |</title><link rel=canonical href=./artificial-intelligence-racial-gender-bias.html><link rel=stylesheet href=https://assets.cdnweb.info/hugo/cleanwhite/css/bootstrap.min.css><link rel=stylesheet href=https://assets.cdnweb.info/hugo/cleanwhite/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=https://assets.cdnweb.info/hugo/cleanwhite/css/zanshang.css><link href=https://cdn.jsdelivr.net/gh/FortAwesome/Font-Awesome@5.15.1/css/all.css rel=stylesheet type=text/css><script src=https://assets.cdnweb.info/hugo/cleanwhite/js/jquery.min.js></script>
<script src=https://assets.cdnweb.info/hugo/cleanwhite/js/bootstrap.min.js></script>
<script src=https://assets.cdnweb.info/hugo/cleanwhite/js/hux-blog.min.js></script></head><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span></button>
<a class=navbar-brand href=./>NovaVibe</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=./categories/blog>blog</a></li><li><a href=./sitemap.xml>Sitemap</a></li><li><a href=./index.xml>RSS</a></li></ul></div></div></div></nav><script>var $body=document.body,$toggle=document.querySelector(".navbar-toggle"),$navbar=document.querySelector("#huxblog_navbar"),$collapse=document.querySelector(".navbar-collapse");$toggle.addEventListener("click",handleMagic);function handleMagic(){$navbar.className.indexOf("in")>0?($navbar.className=" ",setTimeout(function(){$navbar.className.indexOf("in")<0&&($collapse.style.height="0px")},400)):($collapse.style.height="auto",$navbar.className+=" in")}</script><style type=text/css>header.intro-header{background-image:url(/img/home-bg-jeep.jpg)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags></div><h1>Artificial Intelligence Has a Racial and Gender Bias Problem</h1><h2 class=subheading></h2><span class=meta>Posted by
Larita Shotwell
on
Tuesday, June 18, 2024</span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Machines can discriminate in harmful ways.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">I experienced this firsthand<b>, </b>when I was a graduate student at MIT in 2015 and discovered that some facial analysis software couldn’t detect my dark-skinned face until I put on a white mask. These systems are often trained on images of predominantly light-skinned men. And so, I <a href=# rel=noopener>decided to share</a> my experience of the coded gaze, the bias in artificial intelligence that can lead to discriminatory or exclusionary practices.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Altering myself to fit the norm—in this case better represented by a white mask than my actual face—led me to realize the impact of the exclusion overhead, a term I coined to describe the cost of systems that don’t take into account the diversity of humanity. How much does a person have to change themselves to function with technological systems that increasingly govern our lives?</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">We often assume machines are neutral, but they aren’t. My research uncovered large gender and racial bias in AI systems sold by tech giants like IBM, Microsoft, and <a href=# rel=noopener>Amazon</a>. Given the task of guessing the gender of a face, all companies performed substantially better on male faces than female faces. The companies I evaluated had error rates of no more than 1% for lighter-skinned men. For darker-skinned women, the <a href=# rel=noopener>errors soared</a> to 35%. AI systems from leading companies have <a href=# rel=noopener>failed to correctly classify</a> the faces of Oprah Winfrey, Michelle Obama, and Serena Williams. When technology denigrates even these iconic women, it is time to re-examine how these systems are built and who they truly serve.</p><img style=margin:auto;display:block;text-align:center;max-width:100%;height:auto src=https://cdn.statically.io/img/api.time.com/wp-content/uploads/2019/02/oprah-winfrey.png><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">There’s no shortage of headlines highlighting tales of failed machine learning systems that amplify, rather than rectify, sexist hiring practices, racist criminal justice procedures, predatory advertising, and the spread of false information. Though these research findings can be discouraging, at least we’re paying attention now. This gives us the opportunity to highlight issues early and prevent pervasive damage down the line. <a href=# rel=noopener>Computer vision experts</a>,<a href=# rel=noopener> the ACLU,</a> and the<a href=# rel=noopener> Algorithmic Justice League</a>, which I founded in 2016, have all uncovered racial bias in facial analysis and recognition technology. Given what we know now, as well as the history of racist police brutality, there needs to be a moratorium on using such technology in law enforcement—including in equipping drones or police body cameras with facial analysis or recognition software for lethal operations.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">See the <a href=#>2019 Optimists issue</a>, guest-edited by Ava DuVernay.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">We can organize to protest this technology being used dangerously. When people’s lives, livelihoods, and dignity are on the line, AI must be developed and deployed with care and oversight. This is why I launched the<a href=# rel=noopener> Safe Face Pledge</a> to prevent the lethal use and mitigate abuse of facial analysis and recognition technology. Already three companies have agreed to sign the pledge.</p><img style=margin:auto;display:block;text-align:center;max-width:100%;height:auto src=https://cdn.statically.io/img/api.time.com/wp-content/uploads/2019/02/michelle-obama.png><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">As more people question how seemingly neutral technology has gone astray, it’s becoming clear just how important it is to have broader representation in the design, development, deployment, and governance of AI. The underrepresentation of women and people of color in technology, and the under-sampling of these groups in the data that shapes AI, has led to the creation of technology that is optimized for a small portion of the world. Less than 2% of <a href=# rel=noopener>employees in technical roles</a> at Facebook and <a href=# rel=noopener>Google</a> are black. At eight large tech companies <a href=# rel=noopener>evaluated by Bloomberg</a>, only around a fifth of the technical workforce at each are women. I found one government dataset of faces collected for testing that contained<a href=# rel=noopener> 75% men and 80%</a> lighter-skinned individuals and less than 5% women of color—echoing the pale male data problem that excludes so much of society in the data that fuels AI.</p><img style=margin:auto;display:block;text-align:center;max-width:100%;height:auto src=https://cdn.statically.io/img/api.time.com/wp-content/uploads/2019/02/serena-williams.png><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Issues of bias in AI tend to most adversely affect the people who are rarely in positions to develop technology. Being a black woman, and an outsider in the field of AI, enables me to spot issues many of my peers overlooked.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">I am optimistic that there is still time to shift towards building ethical and inclusive AI systems that respect our human dignity and rights. By working to reduce the exclusion overhead and enabling marginalized communities to engage in the development and governance of AI, we can work toward creating systems that embrace full spectrum inclusion.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">In addition to lawmakers, technologists, and researchers, this journey will require storytellers who embrace the search for truth through art and science. Storytelling has the power to shift perspectives, galvanize change, alter damaging patterns, and reaffirm to others that their experiences matter. That’s why art can explore the emotional, societal, and historical connections of algorithmic bias in ways academic papers and statistics cannot. And as long as stories ground our aspirations, challenge harmful assumptions, and ignite change, I remain hopeful.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">See the <a href=#>2019 Optimists issue</a>, guest-edited by Ava DuVernay.</p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmismaKyb6%2FOpmZubWJlgnaEjpqpraGWnrCqrctmoKeslaG5qrPEp5qeZaKWsKqty2aenqaUmr9ursiaqmg%3D</p><hr><ul class=pager><li class=previous><a href=./brit-school-adele-amy-winehouse-turns-30-1235122453.html data-toggle=tooltip data-placement=top title="Brit Beat: Brit School, Home to Adele and Amy Winehouse, Turns 30; Streaming Battle Heads to P">&larr;
Previous Post</a></li><li class=next><a href=./stanley-tucci-still-grieves-his-first-wife-despite-being-happily-remarried-816437.html data-toggle=tooltip data-placement=top title="Stanley Tucci Still Grieves His First Wife Despite Being Happily Remarried">Next
Post &rarr;</a></li></ul></div><div class="col-lg-2 col-lg-offset-0
visible-lg-block
sidebar-container
catalog-container"><div class=side-catalog><hr class="hidden-sm hidden-xs"><h5><a class=catalog-toggle href=#>CATALOG</a></h5><ul class=catalog-body></ul></div></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"><section><hr class="hidden-sm hidden-xs"><h5><a href=./tags/>FEATURED TAGS</a></h5><div class=tags></div></section></div></div></div></article><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"></ul><p class="copyright text-muted">Copyright &copy; NovaVibe 2024<br><a href=https://themes.gohugo.io/hugo-theme-cleanwhite>CleanWhite Hugo Theme</a> |
<iframe style=margin-left:2px;margin-bottom:-5px frameborder=0 scrolling=0 width=100px height=20px src="https://ghbtns.com/github-btn.html?user=zhaohuabing&repo=hugo-theme-cleanwhite&type=star&count=true"></iframe></p></div></div></div></footer><script>function loadAsync(i,t){var n=document,s="script",e=n.createElement(s),o=n.getElementsByTagName(s)[0];e.src=i,t&&e.addEventListener("load",function(e){t(null,e)},!1),o.parentNode.insertBefore(e,o)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,r=$(_containerSelector),a=r.find("h1,h2,h3,h4,h5,h6");return $(e).html(''),a.each(function(){n=$(this).prop("tagName").toLowerCase(),i="#"+$(this).prop("id"),s=$(this).text(),t=$('<a href="'+i+'" rel="nofollow">'+s+"</a>"),o=$('<li class="'+n+'_nav"></li>').append(t),$(e).append(o)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("https://assets.cdnweb.info/hugo/cleanwhite/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://iklan.listspress.com/banner.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://iklan.listspress.com/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>